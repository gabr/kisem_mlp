\section{Observations and Conclusions}
\label{ObservationsConclusions}

As a summary some observations and conclusions related to topic will be presented.

\subsection{Limitations}
\label{Limitations}

Neural networks are perfect in creating mathematical models based on sample set and also without it (unsupervised learning) but they cannot model all functions. For example generation of random value cannot be achieve by neural network. Also choosing the proper sample set is a very difficult task itself and we need to be aware of \textit{overfitting} problem.

\hyperref[sec:Training]{Error Back Propagation} algorithm is very old and simple algorithm. It works but it is not perfect. It not guarantee reaching global minimum just a local one. Its also very sensitive to initial values of neuron weights. The criteria to stop algorithm are also not exactly clear. \\
There exist various modifications of pure \hyperref[sec:Training]{EBP} algorithm which solves some of this problems.

\subsection{Implementation}
\label{Implementation}

Neural network is described by mathematical tools so there is no strict way from equations to implementation.
But complexity of implementation is much lower in compare to theory and benefits of this neural networks algorithms are much larger than implementation effort.

Good advice for implementation is to \textbf{not} follow OOP standards and implement neurons as set of arrays and lists. This makes transition between mathematical description and code very easy, fast and produces less errors.


\subsection{Applications}
\label{Applications}